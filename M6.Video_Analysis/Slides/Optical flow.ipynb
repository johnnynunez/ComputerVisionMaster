{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optical flow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6EbyOVDlXYLMNxsI+53T1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eNC7_2bivIFu"},"source":["<table width=100%>\n","<tr>\n","<td style=\"text-align: right;\" width=100%>\n","<img src=\"https://drive.google.com/uc?export=view&id=1v2B2Eoumm2vyK1Ww1DG_HyBOlMSmfTps\" width=700>\n","</tr>\n","<tr>\n","<td width=100%>\n","<h1><b>Master in Computer Vision - M6</b></h1>\n","<h2><b>Motion examples</b></h2>\n","<br>\n","<h4>2021,2022 - Josep Ramon Morros\n","<br>\n","<a href=\"https://imatge.upc.edu/web/\"> GPI @ IDEAI</a> Research group\n","</h4>\n","</td>\n","</tr>\n","</table>"]},{"cell_type":"markdown","source":["# Motion compensation\n","Lets take the reference image in the example from the slides, and its motion vectors and cnstruct the compensated image:"],"metadata":{"id":"jpdfnPRXnard"}},{"cell_type":"code","source":["import numpy as np\n","import pprint as pp\n","\n","# Reference image\n","ref_ima = [[44,55,37],[21,22,60],[31,44,25]]\n","\n","# Optical flow\n","of_x = [[0,1,-1], [2,-1,-1], [0,0,0]]\n","of_y = [[0,0,0], [1,1,1],[-1,-1,-1]]\n","\n","print (np.array(ref_ima))\n"],"metadata":{"id":"ISTxxdXEnwG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compensated image\n","comp_ima = np.zeros((3,3))\n","\n","# Forward compensation\n","for ii in range(3):\n","    for jj in range(3):\n","        comp_ima[ii+of_y[ii][jj], jj+of_x[ii][jj]] = ref_ima[ii][jj]\n","\n","print (comp_ima)"],"metadata":{"id":"mkxrq-N0nySO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LP6mKUiDG06Y"},"source":["# Parametric motion models examples\n","\n","Download the file of_comp.zip that you will find at the virtual campus and upload it here using the menu on the left (click the folder icon, upload the file). Wait until the upload is complete, it may take a long time"]},{"cell_type":"code","metadata":{"id":"0EEWoDVwHR3O"},"source":["!unzip of_comp.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XieWOnMfw4mL"},"source":["# Install package for optical flow visualization using color coding\n","!pip install flow_vis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ta9N9uKtvVMi"},"source":["import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from flow_vis import flow_to_color"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgwj-3RAHG9n"},"source":["from of_comp.code.display_images import display_image, display_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wRvoY6j5vOXR"},"source":["Define an object in an image (square white box)"]},{"cell_type":"code","metadata":{"id":"aiKtk2clu883"},"source":["# Define an image with an object (square box)\n","ima1 = np.zeros((200,200), dtype=np.uint8)\n","ima1[90:111,90:111] = 255\n","\n","display_image(ima1, 'Object in I(t-dt)', size = 1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QzVRHSWxXoS"},"source":["## Traslational motion model\n"]},{"cell_type":"markdown","metadata":{"id":"Pzj1Kaqbxv57"},"source":["Define the model parameters. The object will move 10 pixels down, 25 pixels right"]},{"cell_type":"code","metadata":{"id":"RFgsT1KJxr6y"},"source":["# Traslation vector\n","b = np.array([10,25])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wRrE4pF2wPgu"},"source":["Apply the traslational model to the object:"]},{"cell_type":"code","metadata":{"id":"q6Cz1qlBwVek"},"source":["# Create empty image\n","ima2 = np.zeros_like(ima1)\n","\n","for ii in range(0,ima1.shape[0]):\n","    for jj in range(0,ima1.shape[1]):\n","        pt  = np.array([ii,jj])\n","\n","        # Check that the pixel belongs to the object\n","        if ima1[ii,jj] == 255:\n","            dst = pt + b\n","            dst = dst.astype(int)\n","\n","            # Check destination is within image borders\n","            if dst[0] >= 0 and dst[0] < ima2.shape[0] and dst[1] >= 0 and dst[1] < ima2.shape[1]:\n","                ima2[dst[0], dst[1]] = ima1[pt[0], pt[1]]\n","\n","display_images(ima1, ima2, 'original', 'Translated object', size=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBrclPpHzpph"},"source":["We can define a function to obtain the motion vectors. For simplicity, it is not a true motion estimation process, we simply assume that all the pixels of the object move with the same translational motion. "]},{"cell_type":"code","metadata":{"id":"4bdrabLVzyPJ"},"source":["def traslational_flow_from_params(params, ima, only_object=False):\n","    h, w = ima.shape[0:2]\n","\n","    if only_object == True:\n","        mask = np.zeros_like(ima1)\n","        mask[ima1 > 0] = 1\n","    else:\n","        mask = np.ones_like(ima1)\n","    \n","    of_u = np.full_like(ima1, params[0])\n","    of_v = np.full_like(ima1, params[1])\n","\n","    of_u = of_u * mask\n","    of_v = of_v * mask\n","\n","    return np.stack([of_u, of_v], axis=2).astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DNqccBHu84Hd"},"source":["We can obtain the optical flow (for each object point) by applying the transformation to the object:"]},{"cell_type":"code","metadata":{"id":"H5LhI8f088Nk"},"source":["tras_of = traslational_flow_from_params(b, ima1, only_object=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cI7UNsABKm62"},"source":["To visualize the optical flow, we can use a color based encoding (from package flow_vis). "]},{"cell_type":"code","metadata":{"id":"AASIMvBJFg6k"},"source":["# Create the optical flow color encoding legend\n","def flow_with_legend(of):\n","    h,w = of.shape[0:2]\n","    wsize = np.array([2*max(h,w)+1, 2*max(h,w)+1], dtype=int)\n","    wheel = np.zeros(wsize, dtype=np.float32)\n","\n","    xpos = np.array([np.arange(wsize[1]),]*wsize[0], dtype=np.float32)   - wsize[1]/2\n","    ypos = np.array([np.arange(wsize[0]),]*wsize[1], dtype=np.float32).T - wsize[0]/2\n","\n","    mvects = np.stack([xpos,-ypos], axis=2)\n","    modulus = np.sqrt(xpos*xpos + ypos*ypos)\n","    modulus = modulus / modulus[0,wsize[0]//2]\n","\n","    mask_wheel = np.zeros(wsize, dtype=np.float32)\n","    mask_wheel[modulus <=1] = 1\n","    mask_wheel = np.stack([mask_wheel,mask_wheel],axis=2)\n","\n","    # Generate the legend image\n","    of_legend = flow_to_color(mvects*mask_wheel, convert_to_bgr=True)\n","\n","    # Encode the flow using the color representation\n","    cc = flow_to_color(of, convert_to_bgr=True)\n","\n","\n","    plt.figure(figsize=(6, 4))\n","    G = gridspec.GridSpec(2, 3)\n","    axes_1 = plt.subplot(G[:, 0:2])\n","    axes_1.imshow(cc)\n","    axes_1.set_title('Optical Flow')\n","\n","    axes_2 = plt.subplot(G[:, 2])\n","    axes_2.imshow(of_legend, extent=[-wsize[0]/2, wsize[0]/2, wsize[0]/2, -wsize[0]/2])\n","    axes_2.set_title ('Color code')\n","\n","    plt.tight_layout()\n","\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQckTlOkK6Q2"},"source":["#cc = flow_to_color(of, convert_to_bgr=True)\n","#plt.imshow(cc)\n","flow_with_legend(tras_of)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8pCgwyjyEnt"},"source":["## Affine motion model"]},{"cell_type":"markdown","metadata":{"id":"zd8H345jyLdK"},"source":["Define the model parameters. The output will be:\n","\n","```\n","x' = M11*x + M12*y + M13\n","y' = M21*x + M22*y + M23\n","```"]},{"cell_type":"code","metadata":{"id":"eMb6ZR_-xm8c"},"source":["# Affine model. Define the direct transformation matrix\n","Md = np.array([[ -1.06065,   0.7071,  167.1745 ],\n","               [  1.06065,   0.7071,  -87.3815 ]])\n","\n","print (Md)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjwYfUVzpJfF"},"source":["Apply the affine model to the object. Note the holes in the result."]},{"cell_type":"code","metadata":{"id":"lTu4cHNdpEC7"},"source":["# Create empty image\n","ima2 = np.zeros_like(ima1)\n","\n","for ii in range(0,ima1.shape[0]):\n","    for jj in range(0,ima1.shape[1]):\n","\n","        # Check that the pixel belongs to the object\n","        if ima1[ii,jj] == 255:\n","            dst_x = Md[0,0]*jj + Md[0,1]*ii + Md[0,2]\n","            dst_y = Md[1,0]*jj + Md[1,1]*ii + Md[1,2]\n","            dst   = np.array([dst_y, dst_x]).astype(int)\n","\n","            dst = dst.astype(int)\n","\n","            # Check destination is within image borders\n","            if dst[0] >= 0 and dst[0] < ima2.shape[0] and dst[1] >= 0 and dst[1] < ima2.shape[1]:\n","                ima2[dst[0], dst[1]] = ima1[ii, jj]\n","\n","display_images(ima1, ima2, 'original', 'Translated object', size=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RU5eCHN03XYZ"},"source":["Backward compensation. To find the inverse transform, we have to invert matrix Md. As the matrix is not square, a row at the bottom with ```[0.0, 0.0, 1.0]``` is added before inverting, and then the last row of the inverted matrix is discarded."]},{"cell_type":"code","metadata":{"id":"o6k3hmRS3ImB"},"source":["hh = np.array([[0.0, 0.0, 1.0]])\n","\n","# Inverse transformation matrix for the backward compensation\n","Md3 = np.concatenate ([Md,hh], axis=0)\n","Mi = np.linalg.inv(Md3)[0:2,:]\n","\n","print ('Original matrix in homogeneous coordinates')\n","print (Md3)\n","print ('Invertex transformation matrix')\n","print (Mi)\n","print ('--------------------------------')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create empty image\n","ima2 = np.zeros_like(ima1)\n","\n","for ii in range(0,ima1.shape[0]):\n","    for jj in range(0,ima1.shape[1]):\n","        ori_x = Mi[0,0]*jj + Mi[0,1]*ii + Mi[0,2]\n","        ori_y = Mi[1,0]*jj + Mi[1,1]*ii + Mi[1,2]\n","        ori   = np.array([ori_y, ori_x]).astype(int)\n","\n","        if ori[0] >= 0 and ori[0] < ima1.shape[0] and ori[1] >= 0 and ori[1] < ima1.shape[1]:\n","            ima2[ii, jj] = ima1[ori[0], ori[1]]\n","\n","display_images(ima1, ima2, 'original', 'image 2', size=1.0)"],"metadata":{"id":"2gZWmnkfqU1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAc9u5qZsH7t"},"source":["Observe that there are no holes in the backward compensation."]},{"cell_type":"code","metadata":{"id":"Clc_kQAE1R45"},"source":["def affine_flow_from_params(params, ima, only_object=False):\n","    h, w = ima.shape[0:2]\n","\n","    xpos = np.array([np.arange(w),]*h, dtype=np.float32)\n","    ypos = np.array([np.arange(h),]*w, dtype=np.float32).T\n","\n","    of_u = (params[0,0]*xpos + params[0,1]*ypos + params[0,2] * np.ones_like(xpos)) - xpos\n","    of_v = (params[1,0]*xpos + params[1,1]*ypos + params[1,2] * np.ones_like(ypos)) - ypos\n","\n","    if only_object == True:\n","        mask = np.zeros_like(ima1)\n","        mask[ima1 > 0] = 1\n","    else:\n","        mask = np.ones_like(ima1, dtype=float32)\n","\n","    of_u = of_u * mask\n","    of_v = of_v * mask\n","\n","    return np.stack([of_u,of_v], axis=2).astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BJ1gBN9UVJX"},"source":["We can obtain the optical flow (for each object point) by applying the transformation to the object:"]},{"cell_type":"code","metadata":{"id":"7gVPmjGygGbr"},"source":["affine_of = affine_flow_from_params(Md, ima1, only_object=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ERRQYYEUW_D"},"source":["Visualize the optical flow:"]},{"cell_type":"code","metadata":{"id":"PWemtp3Xh9je"},"source":["flow_with_legend(affine_of)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji3AH-ImaQj5"},"source":["Visualize (some of) the motion vectors:"]},{"cell_type":"code","metadata":{"id":"99RoEnvlU0VS"},"source":["X = np.arange(0, affine_of.shape[1], 1)\n","Y = np.arange(0, affine_of.shape[0], 1)\n","x,y = np.meshgrid(X,Y)\n","\n","# Points over the object\n","Xo = np.arange(90, 111, 10)\n","Yo = np.arange(90, 111, 10)\n","xo,yo = np.meshgrid(Xo,Yo)\n","\n","U = np.zeros(affine_of.shape[0:2])\n","V = np.zeros(affine_of.shape[0:2])\n","\n","U[90:111:10, 90:111:10] = affine_of[90:111:10, 90:111:10, 0]\n","V[90:111:10, 90:111:10] = affine_of[90:111:10, 90:111:10, 1]\n","plt.imshow(np.logical_or(ima1,ima2))\n","plt.quiver(X, Y, U, V, color = 'r', angles='xy', scale_units='xy', scale=1,\n","               pivot = 'tail', headwidth=1, headlength =2)"],"execution_count":null,"outputs":[]}]}